{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Look at Gravity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import holoviews as hv\n",
    "from holoviews import dim, opts\n",
    "import hvplot.dask\n",
    "hv.extension('bokeh')\n",
    "import glob, os\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def dateparse (date_string):\n",
    "    return datetime.datetime.strptime(date_string, '%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "def dateparseSPAIN (date_string):\n",
    "    return datetime.datetime.strptime(date_string, '%Y-%m-%d-%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head ~jovyan/data/bravoseis/gravity/gravimetro_bruto/21012019.gravimetro_bruto.proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jovyan/data/bravoseis_data/SADO/jan_2019/gravimetro_bruto.proc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head ~/Dropbox/QueensCollege/Research/BransfieldStrait/data/grav/raw/21012019.gravimetro_bruto.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Gravity Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = '/home/jovyan/data/bravoseis_data/SADO/jan_2019/gravimetro_bruto.proc/' # use your path\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"*.proc\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, parse_dates=True, date_parser=dateparse, index_col='fecha',\n",
    "                       dtype = {'Date': object,'status': np.float64,\n",
    "                                'gravimetria_bruta': np.float64, 'spring_tension': np.float64,\n",
    "                                'longitud': np.float64, 'latitud': np.float64,\n",
    "                                'velocidad': np.float64,'rumbo': np.float64 }) for f in all_files)\n",
    "\n",
    "concatenated_df   = pd.concat(df_from_each_file, ignore_index=False)\n",
    "df_grav   = concatenated_df.sort_values(by='fecha_telegrama')\n",
    "df_grav['date']=df_grav.index.date\n",
    "df_grav['time']=df_grav.index.time\n",
    "df_grav['survey']= np.nan # Survey Part\n",
    "df_grav['loca'] = np.nan # Survey Location \n",
    "df_grav['line']= np.nan # Line Number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grav.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Bathy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/jovyan/data/bravoseis_data/SADO/jan_2019/posicion.proc/01012019.posicion.proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = '/home/jovyan/data/bravoseis_data/SADO/jan_2019/posicion.proc/' # use your path\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"*.proc\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_Bath = (pd.read_csv(f, parse_dates=True, date_parser=dateparse, index_col='fecha',\n",
    "                       dtype = {'Date': object,'longitud': np.float64,\n",
    "                                'latitud': np.float64, 'rumbo': np.float64,\n",
    "                                'velocidad': np.float64, 'profundidad': np.float64,\n",
    "                                'cog': np.float64,'sog': np.float64 }) for f in all_files)\n",
    "\n",
    "concatBathy_df   = pd.concat(df_from_each_Bath, ignore_index=False)\n",
    "df_bath   = concatBathy_df.sort_values(by='fecha_telegrama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bath.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bath.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/jovyan/data/bravoseis_data/MAGNETOMETRO/190121_0001.mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Mag Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = '/home/jovyan/data/bravoseis_data/MAGNETOMETRO/' # use your path\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"*.proc\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, parse_dates=True, date_parser=dateparse, index_col='fecha',\n",
    "                       dtype = {'Date': object,'status': np.float64,\n",
    "                                'gravimetria_bruta': np.float64, 'spring_tension': np.float64,\n",
    "                                'longitud': np.float64, 'latitud': np.float64,\n",
    "                                'velocidad': np.float64,'rumbo': np.float64 }) for f in all_files)\n",
    "\n",
    "concatenated_df   = pd.concat(df_from_each_file, ignore_index=False)\n",
    "df_grav   = concatenated_df.sort_values(by='fecha_telegrama')\n",
    "df_grav['date']=df_grav.index.date\n",
    "df_grav['time']=df_grav.index.time\n",
    "df_grav['survey']= np.nan # Survey Part\n",
    "df_grav['loca'] = np.nan # Survey Location \n",
    "df_grav['line']= np.nan # Line Number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(df_bath, df_grav,how='inner', indicator=True, left_index=True, right_index=True, suffixes=('_B', '_G'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gravMerge = pd.DataFrame()\n",
    "df_gravMerge = test[test['_merge'] == 'both']\n",
    "del df_gravMerge['_merge']\n",
    "df_gravMerge['longitud'] = (df_gravMerge['longitud_B'] + df_gravMerge['longitud_G'])/2\n",
    "df_gravMerge['latitud'] = (df_gravMerge['latitud_B'] + df_gravMerge['latitud_G'])/2\n",
    "df_gravMerge['rumbo'] = (df_gravMerge['rumbo_B'] + df_gravMerge['rumbo_G'])/2\n",
    "df_gravMerge['velocidad'] = (df_gravMerge['velocidad_B'] + df_gravMerge['velocidad_G'])/2\n",
    "del df_gravMerge['longitud_B']\n",
    "del df_gravMerge['latitud_B']\n",
    "del df_gravMerge['rumbo_B']\n",
    "del df_gravMerge['velocidad_B']\n",
    "del df_gravMerge['fecha_telegrama_B']\n",
    "del df_gravMerge['longitud_G']\n",
    "del df_gravMerge['latitud_G']\n",
    "del df_gravMerge['rumbo_G']\n",
    "del df_gravMerge['velocidad_G']\n",
    "del df_gravMerge['fecha_telegrama_G']\n",
    "del df_gravMerge['status']\n",
    "df_gravMerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail bravoseis_tables.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineNumbers = (pd.read_csv('bravoseis_tables.csv', parse_dates=[3, 4], date_parser=dateparseSPAIN))\n",
    "df_lineNumbers.columns = ['survey','loca','line',\n",
    "                     's_time','e_time']\n",
    "#df_lineNumbers.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lineNumbers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df_lineNumbers.e_time, format = \"%Y-%m-%d-%H:%M:%S\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_lineNumbers.iterrows():\n",
    "    mask = (df_gravMerge.index > row.s_time) & (df_gravMerge.index <= row.e_time)\n",
    "    df_gravMerge.survey[mask]= row.survey\n",
    "    df_gravMerge.loca[mask]= row.loca\n",
    "    df_gravMerge.line[mask]= row.line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravimetria = df_gravMerge.dropna()\n",
    "gravimetria.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravimetria = gravimetria[gravimetria.loca != 'EdifaceA'];\n",
    "gravimetria = gravimetria[gravimetria.loca != 'Transit'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'RIF12'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'RIF11'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'RIF11b'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'RIF10'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'RIF04'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'RIF03'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'RIF02'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'ORK10'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'ORK17'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'ORK02']; #Start of line for OR_2 at 08:59 UTC. The streamer is completely outside the line (Feather angle of -13 º in the good part of the line). At 1300 UTC end of line for OR_2\n",
    "gravimetria = gravimetria[gravimetria.line != 'ORK05'];\n",
    "gravimetria = gravimetria[gravimetria.line != 'ORK18b'];#We begin the Turn B with the line OR18. There is a large iceberg 4 km away, at the moment we will not deviate from the line. At 15:10 (UTC) the guns have tangled with the streamer. They stopped the acquisition of data. At 18:05 we returned to the initial point to redo the survey of the line 2300 UTC bad weather, large waves make multibeam data poor quality. \n",
    "gravimetria = gravimetria[gravimetria.line != 'T10'];\n",
    "\n",
    "\n",
    "gravimetria = gravimetria.rename(columns={\"profundidad\": \"Depth\", \"longitud\": \"Longitude\", \"latitud\": \"Latitude\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravimetria.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude Corrections \n",
    "g(phi) = 978.0327(1+0.0052792 sin^2(phi) - 0.0000232sin^4(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-air\n",
    "* F1 = F2 = G (m1m2/r^2)\n",
    "* G = 6.670 x 10 N-m^2/kg^2\n",
    "* r = distance between point masses \n",
    "* F1, F2 = force of attraction \n",
    "* F= 0.3086 mGal/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B(rho) = 2*pi*G*rho_Bouguer*h\n",
    "* rho_Bouguer = Bouguer Density in kg/m^3\n",
    "* h = elevation above sea level in meters\n",
    "#### A 2.67 g/cm^3 Bouguer density is commonly used as a good average density for continental crust. Given a Bouguer density, we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gravimetria.to_csv('gravimetria.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gravimetria.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravimetria.hvplot.points('fecha', 'gravimetria_bruta', color='gravimetria_bruta',\n",
    "                             cmap='colorwheel', size=.5,\n",
    "                             hover_cols=['cog', 'line'], title= 'proc_gravity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravimetria.hvplot.scatter('Longitude', 'Latitude', \n",
    "                      height=500, \n",
    "                      color ='gravimetria_bruta', \n",
    "                      cmap='colorwheel', \n",
    "                      size=50, \n",
    "                      hover_cols=['line'], title= 'proc_gravity subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav = pd.DataFrame()\n",
    "df_minuteGrav['gravimetria_bruta'] = gravimetria.gravimetria_bruta.resample('min').mean()\n",
    "df_minuteGrav['lon'] = gravimetria.Longitude.resample('min').mean()\n",
    "df_minuteGrav['lat'] = gravimetria.Latitude.resample('min').mean()\n",
    "df_minuteGrav['depth'] = gravimetria.Depth.resample('min').mean()\n",
    "df_minuteGrav.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav2.hvplot.points('index', 'proc_gravity', color='proc_gravity',\n",
    "                             cmap='colorwheel', size=.5,\n",
    "                             hover_cols=['cog'], title= 'proc_gravity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav.hvplot.scatter('lon ', 'lat', \n",
    "                      height=500, \n",
    "                      color ='gravimetria_bruta', \n",
    "                      cmap='colorwheel', \n",
    "                      size=50, \n",
    "                      hover_cols=['depth'], title= 'proc_gravity subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gravity is measured in MGals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gravMerge['eotvos'] = 4.040 * df_gravMerge['sog'].values * df_gravMerge['cog'].apply(np.sin)* df_gravMerge['latitud'].apply(np.cos) + (0.001211 * df_gravMerge['sog']**2 )\n",
    "df_gravMerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latitude Correction \n",
    "https://rallen.berkeley.edu/teaching/F04_GEO594_IntroAppGeophys/Lectures/L03_GravCorrAnalysis.pdf\n",
    "#### Geodetic Reference System (GRS-1967) formula\n",
    "#### gφ =9.780318(1+0.0053024sin2φ−0.0000059sin2 2φ) m/s2\n",
    "\n",
    "## Bouguer correction\n",
    "\n",
    "#### Accounts for rock thickness between current and base station elevation\n",
    "#### Treat the rock as an infinite horizontal slab:\n",
    "#### CB = 0.000419∆hρ where ∆h is in m and ρ is in km/m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gravMerge.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav2=df_minuteGrav.loc['2019-01-20 00:00:00':'2019-01-24 00:00:00']\n",
    "df_temp=df_minuteGrav.loc['2019-01-26 21:00:00':'2019-02-05 23:58:00']\n",
    "df_minuteGrav2=df_minuteGrav2.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav2.hvplot.points('lon', 'lat', \n",
    "                      height=500, \n",
    "                      color='proc_gravity', \n",
    "                      cmap='colorwheel', \n",
    "                      size=3, \n",
    "                      hover_cols=['depth'], title= 'proc_gravity',\n",
    "                      fontsize={'title': 16, 'labels': 14, 'xticks': 12, 'yticks': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_minuteGrav2.hvplot.heatmap(x='lon', y='lat', C='proc_gravity', reduce_function=np.mean, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to notice:\n",
    "1. The depth signiature is visable\n",
    "2. Examine crossing paths... there is a directioal dependence to our readings related to ship direction. \n",
    "3. Is the difference between these lines just the ETVOS correction or are their other corrections that need to be applied? \n",
    "4. Whould you please share the processing stream? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav2.hvplot.points('index', 'proc_gravity', color='proc_gravity',\n",
    "                             cmap='colorwheel', size=.5,\n",
    "                             hover_cols=['cog'], title= 'proc_gravity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = df_minuteGrav2[\"lat\"] < -62.44    \n",
    "cond2 = df_minuteGrav2[\"lat\"] > -62.45\n",
    "cond3 = df_minuteGrav2[\"lon\"] > -58.42\n",
    "cond4 = df_minuteGrav2[\"lon\"] < -58.36\n",
    "\n",
    "df_minuteGrav3 = df_minuteGrav2[cond1 & cond2 & cond3 & cond4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_minuteGrav3['eotvos']\n",
    "del df_minuteGrav3['grav_corr']\n",
    "df_minuteGrav3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav3.hvplot.scatter('lon', 'lat', \n",
    "                      height=500, \n",
    "                      color='proc_gravity', \n",
    "                      cmap='colorwheel', \n",
    "                      size=50, \n",
    "                      hover_cols=['depth'], title= 'proc_gravity subset').opts(bgcolor= grey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minuteGrav3.to_csv('proc_gravity_subset.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gravitational constant in SI units :math:`m^3 kg^{-1} s^{-1}`\n",
    "## GRAVITATIONAL_CONST = 0.00000000006673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### If terrain corrections (see below) are not applied, the term simple Bouguer anomaly is used. If they have, the term complete Bouguer anomaly is used. A second order correction to account for the curvature of the Earth is often added to this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipsoid = get_ellipsoid()\n",
    " #Convert latitude to radians\n",
    "    latitude_rad = np.radians(latitude)\n",
    "     prime_vertical_radius = ellipsoid.semimajor_axis / np.sqrt(1 - ellipsoid.first_eccentricity ** 2 * np.sin(latitude_rad) ** 2)\n",
    "        # Instead of computing X and Y, we only comupute the projection on the XY plane:\n",
    "        # xy_projection = sqrt( X**2 + Y**2 )\n",
    " xy_projection = (height + prime_vertical_radius) * np.cos(latitude_rad)\n",
    " z_cartesian = (height + (1 - ellipsoid.first_eccentricity ** 2) * prime_vertical_radius) * np.sin(latitude_rad)\n",
    " radius = np.sqrt(xy_projection ** 2 + z_cartesian ** 2)\n",
    " geocentric_latitude = 180 / np.pi * np.arcsin(z_cartesian / radius)\n",
    "\n",
    "    return geocentric_latitude, radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "1. Seismic Velocites can be converted to density using the Carlson and Raskin [1984] velocity/ density relation for oceanic rocks that takes into account the porosity of layer 2. This relationshp can be used to approximate the gravitatational effects of the axial valley topography. \n",
    "\n",
    "2. Hooft 2000 suspended the density model from the bathymetry forllowing the method of Canales et al. [2000]\n",
    "\n",
    "3. To avoid spatial aliasing, density interfaces were mirrored on all sides using the spectral method of Parker 1972\n",
    "\n",
    "4. The crustal vravity signature and the mantle density variations due to the thermal structure of three-dimensional, plate driven mantle flow [Phipps Morgan and Forsyth, 1988] were subtracted from the observed free-air gravity anomaly to obtain the residual crustal Bouguer anomaly. \n",
    "\n",
    "5.  Marjanovic_2011 - Can we see evidence of Orca Volcano low mantle Bouguer gravity anomalies [Ito and Lin, 1995; Canales et al., 2002; Eysteinsson and Gunnarsson, 1995]?\n",
    "\n",
    "6. Two‐ dimensional forward gravity modeling is conducted with the following primary goals: (1) to assess whether the axial density distributions inferred from gravity data are consistent with thicker crust beneath the Cleft and Endeavor segments, two ridge segments of proposed melt anomaly influ- ence, (2) to assess whether additional anomalous densities (i.e., in the mantle) are required to account for axial gravity anomalies, and (3) to evaluate constraints from gravity data on crustal structure of pseudofault zones and the origin of Moho travel time anomalies observed adjacent to pseudofaults.\n",
    "\n",
    "[6] With gravity data, crustal thickness variations cannot be uniquely distinguished from variations in crust and/or mantle densities and the common approach is to evaluate a range of plausible models. With a few exceptions, constraints on crustal thickness from seismic studies are not typically available in prior gravity modeling studies of oce- anic crustal structure. Here, the available constraints from seismic data for the structure of uppermost crust (layer 2a) and for Moho reflection are used and a suite of models of varying middle‐to‐lower crustal structure are constructed. We investigate ridge axis structure using models of constant den- sity and thickness crust, constant density and var- iable thickness crust from the seismic reflection data, varying densities within the crust due to plate cooling away from the ridge axis, and varying densities within the mantle due to plate cooling.\n",
    "\n",
    "Crustal structure at pseudofaults is investigated using best fit models of variable crustal densities given the seismic constraints on crustal thickness. The gravity models support the presence of thicker crust at both Cleft and Endeavor segments and require a broader zone of low densities in the underlying mantle beneath all segments. Preferred models for the ridge flank pseudofaults indicate local zones of thinner and thicker crust and higher densities. The crustal structure models are inter- preted in terms of implications for present‐day accretion processes along the JdF Ridge and at propagating ridge tips in the past.\n",
    "\n",
    "As described previously, the primary goals of our study are to use gravity data to further inves- tigate anomalies in crustal structure inferred from seismic reflection data. Two‐dimensional forward gravity modeling along the three ridge flank pro- files is conducted. The GM‐SYS gravity/magnetic modeling software [Won and Bevis, 1987], pro- fessional basic version is used. The GM‐SYS package uses the method of Talwani et al. [1959] to calculate the gravitational attraction of two‐ dimensional bodies of arbitrary shape approxi- mated by an n‐sided polygon and constant density. All GM‐SYS models are extended to ±30,000 km (“infinity”) in the X direction to eliminate edge effects. Uniform structure perpendicular to the pro- file orientation is assumed with the 2‐D approxi- mation. While this assumption is well justified for the ridge axis and flanks where profile orientation is perpendicular to the dominant structural trends, it is less appropriate for the pseudofaults, which are oblique to the profile trend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
